#+STARTUP:showall

* running athena in parallel
  
  you can speed up your simulations by running athena in /parallel/,
  basically splitting up your simulation domain into N equal-sized
  chunks and then running each one on a separate processor.  on each
  of these chunks, the boundary conditions come from the simulation
  running on the neighboring processor.  athena uses something called
  MPI (message passing interface) to communicate information among
  processors.  i would recommend /not/ looking into the details of MPI
  unless you have to.

** configuring and compiling
   to make use of multiple processors, you first have to re-compile
   athena with MPI enabled
   
   #+BEGIN_EXAMPLE
   $ ./configure --with-gas=hydro --with-problem=... --enable-mpi
   $ make all
   #+END_EXAMPLE
   
   (you may need to install MPI before doing this... if =which mpirun=
   doesn't give you anything, try =$ brew install open-mpi=)

** setting up the simulation
   next, you need to tell athena how many processors to use.  this
   happens via the NGrid_x* entries in the =athinput= file.  for
   example:

   #+BEGIN_EXAMPLE
   <domain1>
   level     =  0
   Nx1       =  8192
   x1min     = -2.5e3
   x1max     =  2.5e3
   bc_ix1    =  4
   bc_ox1    =  4
   
   
   Nx2       =  8192
   x2min     = -2.5e3
   x2max     =  2.5e3
   bc_ix2    =  4
   bc_ox2    =  4
   
   Nx3       =  1
   x3min     = -0.5
   x3max     =  0.5
   bc_ix3    =  4
   bc_ox3    =  4
   
   NGrid_x1  =  16
   NGrid_x2  =  32
   NGrid_x3  =  1
   #+END_EXAMPLE

   this runs a (1892)^2 simulation on 16*32 = 512 processors.  i've
   used this for some recent simulations on a supercomputer.  you
   won't have 512 processors on your laptops, so maybe try 
   ~NGrid_x1 = NGrid_x2 = 2~ to begin with.

   when you chose your own parameters, remember that the "surface" of
   cells on your domain need to be communicated as boundary conditions
   at the interfaces among processors.  communication has to happen at
   every time-step, and can easily become a bottleneck if you're not
   careful.  so you don't want to make the sub-domains too small, and
   you want to make them as "square" as possible to minimize the
   surface area to volume ratio.

   at the same time, super-computers are organized into hierarchical
   "nodes," each of which contains some number of processors.  on the
   =stampede= computer, each node contains 16 processors, so we need
   the total number we ask for to be an integer multiple of 16.

   (if you know the distinction between /cores/ and /processors/, i'm
   really talking about cores here.)

** running your simulation
   to run your simulation, you need to do it through MPI:
   #+BEGIN_EXAMPLE
   $ mpirun -np 4 ./athena -i athinput
   #+END_EXAMPLE

   again, it's best not to go into the details of what happens
   here... you shouldn't need it.  this example runs on 4 processors,
   which needs to be the same as the total number you ask for in the
   =athinput= file from the last section.

   you'll notice that athena makes four new directories, =id0=, =id1=,
   =id2=, and =id3=.  each one contains =vtk= files with data from its
   corresponding processor.  in order to read these files into
   mathematica, you first need to join these files.

** joining VTK files
   athena provides a small c program for joining vtk files.  i've
   copied it in this directory.  to use it, first compile it:
   #+BEGIN_EXAMPLE
   $ gcc -o join_vtk.x join_vtk.c -lm
   #+END_EXAMPLE

   this makes an executable called =join_vtk.x=.  you can run it
   directly, but i've made a wrapper program called =join-vtk.rb=
   which should make it easier for you.  in the simulation directory,
   run
   #+BEGIN_EXAMPLE
   $ ruby join-vtk.rb
   #+END_EXAMPLE
   and it will make a directory called =merged= with all of the merged
   vtk files.  you can go ahead an analyze these files in the same way
   you've been working with the files from your single-processor
   ("serial") simulations.


* TODO restart files

* TODO batch system on supercomputers
